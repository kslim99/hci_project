Given relatively large depth of the network, the ability
to propagate gradients back through all the layers in an
effective manner was a concern. The strong performance
of shallower networks on this task suggests that the features produced by the layers in the middle of the network
should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, discrimination
in the lower stages in the classifier was expected. This was
thought to combat the vanishing gradient problem while providing regularization. These classifiers take the form
of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network
with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded. Later control experiments have
shown that the effect of the auxiliary networks is relatively
minor (around 0.5%) and that it required only one of them
to achieve the same effect.
The exact structure of the extra network on the side, including the auxiliary classifier, is as follows:

• An average pooling layer with 5×5 filter size and
stride 3, resulting in an 4×4×512 output for the (4a),
and 4×4×528 for the (4d) stage.
• A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.
• A fully connected layer with 1024 units and rectified
linear activation.
• A dropout layer with 70% ratio of dropped outputs.
• A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but
removed at inference time).
A schematic view of the resulting network is depicted in
Figure 3.
